{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "gw8jsQBPMagl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/lm-evaluation-harness\n",
        "!git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd /content/lm-evaluation-harness && pip install -e ."
      ],
      "metadata": {
        "id": "trMCIk6vM42X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdOn-wqqMV_7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_16bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    device_map=\"auto\",\n",
        "    # quantization_config=quantization_config,\n",
        "    dtype=\"auto\"\n",
        ")\n",
        "model_16bit.model.decoder.layers[-1].final_layer_norm.weight.dtype"
      ],
      "metadata": {
        "id": "kBOL_B6MUS6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_parameters(model):\n",
        "\n",
        "  for params in model.named_parameters():\n",
        "    print(\"Paramters name:\", params[0], \" | \", \"dtype: \", params[1].dtype)"
      ],
      "metadata": {
        "id": "-wm7le0kMpYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversion to 8bit using LLM.int8() method"
      ],
      "metadata": {
        "id": "HLZF6TLbOBE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "KdtGQaj0OpR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eval fp16 model"
      ],
      "metadata": {
        "id": "qsXQuUkW07tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=facebook/opt-350m \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "cVlu7NgnSs1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a2a18f1"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer.push_to_hub(\"YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-threshold-5-llmhead-fp16\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval 8bit model"
      ],
      "metadata": {
        "id": "muHaV0N709uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto \\\n",
        "    --output_path ./results \\\n",
        "    --wandb_args project=\"SmolQuant\",entity=\"rentio\""
      ],
      "metadata": {
        "id": "nQcyiTO41NFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skipping lm head for int8"
      ],
      "metadata": {
        "id": "I8EL7mdKkX7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Extract parameters\n",
        "old_layer = model_8bit.model.decoder.project_out\n",
        "\n",
        "# Create a new fp16 Linear layer with same shape\n",
        "new_layer = torch.nn.Linear(old_layer.in_features, old_layer.out_features, bias=old_layer.bias is not None)\n",
        "new_layer = new_layer.to(torch.float16)\n",
        "\n",
        "# Copy weights (convert to fp16)\n",
        "new_layer.weight.data = old_layer.weight.data.clone().to(torch.float16)\n",
        "print(new_layer.weight.data.dtype)\n",
        "if old_layer.bias is not None:\n",
        "    new_layer.bias.data = old_layer.bias.data.clone().to(torch.float16)\n",
        "\n",
        "# Replace in model\n",
        "model_8bit.model.decoder.project_out = new_layer\n",
        "\n",
        "# model_8bit.model.decoder.project_out.weight.data = model_8bit.model.decoder.project_out.weight.data.to(torch.float16).clone()\n"
      ],
      "metadata": {
        "id": "rAqjG9U_nhqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "# model_8bit.model.decoder.project_out = model_8bit.model.decoder.project_out.clone().to(torch.float16).  # won't work cus the layer Linear8bitLt does not allow the dtype conversion to be successful"
      ],
      "metadata": {
        "id": "y8kzfHLUka8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_parameters(model_8bit)"
      ],
      "metadata": {
        "id": "LkUGBMuAO4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking the memory before and after quantization\n"
      ],
      "metadata": {
        "id": "a4xeym-51pvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mem_16bit = model_16bit.get_memory_footprint() / 1e9"
      ],
      "metadata": {
        "id": "R737jOXsaS1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mem_8bit = model_8bit.get_memory_footprint() / 1e9"
      ],
      "metadata": {
        "id": "kRkKz7ZVaVPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Memory saved ratio"
      ],
      "metadata": {
        "id": "ZEe0JBexat6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(mem_8bit / mem_16bit)*100"
      ],
      "metadata": {
        "id": "843jvYMkavg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uploading to hf\n"
      ],
      "metadata": {
        "id": "--OWSB5h1w58"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "168c2329"
      },
      "source": [
        "!pip install huggingface_hub -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75c42576"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1c093c9"
      },
      "source": [
        "model_8bit.push_to_hub(\"facebook-opt-350m-8bit-llm.int8-llmhead-fp162\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evals (contd)"
      ],
      "metadata": {
        "id": "_MoYV-Us100y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model! (8bit without lm_head being in fp16)"
      ],
      "metadata": {
        "id": "CXH4Uqj8PKd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-llmhead-fp162 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "XkFvzpG8PO2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##changing the threshold"
      ],
      "metadata": {
        "id": "8gru7AnK2pB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"facebook/opt-350m\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    llm_int8_threshold=8.0,\n",
        "    load_in_8bit=True\n",
        "    # llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "model_8bit_with_thresh_8 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "wjWj8cLi2rIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_parameters(model_8bit_with_thresh_8) #basically the project_out wont be converted to int8 anyways"
      ],
      "metadata": {
        "id": "WylxGibyVGMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit_with_thresh_8.push_to_hub(\"facebook-opt-350m-8bit-llm.int8-threshold-8\")"
      ],
      "metadata": {
        "id": "2HGc4bbv26su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-threshold-8 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "KhrnfeLL24LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing threhsold (8) with lm_head to 8bit"
      ],
      "metadata": {
        "id": "crPbKblm7-M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"facebook/opt-350m\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    llm_int8_threshold=5.0,\n",
        "    load_in_8bit=True\n",
        "    # llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "model_8bit_with_thresh_8 = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "zpYzncjY8HJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Extract parameters\n",
        "old_layer = model_8bit.model.decoder.project_out\n",
        "\n",
        "# Create a new fp16 Linear layer with same shape\n",
        "new_layer = torch.nn.Linear(old_layer.in_features, old_layer.out_features, bias=old_layer.bias is not None)\n",
        "new_layer = new_layer.to(torch.float16)\n",
        "\n",
        "# Copy weights (convert to fp16)\n",
        "new_layer.weight.data = old_layer.weight.data.clone().to(torch.float16)\n",
        "print(new_layer.weight.data.dtype)\n",
        "if old_layer.bias is not None:\n",
        "    new_layer.bias.data = old_layer.bias.data.clone().to(torch.float16)\n",
        "\n",
        "# Replace in model\n",
        "model_8bit.model.decoder.project_out = new_layer\n",
        "\n",
        "# model_8bit.model.decoder.project_out.weight.data = model_8bit.model.decoder.project_out.weight.data.to(torch.float16).clone()\n"
      ],
      "metadata": {
        "id": "RqVrkX4L8IeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_8bit.push_to_hub(\"facebook-opt-350m-8bit-llm.int8-threshold-5-llmhead-fp16\")"
      ],
      "metadata": {
        "id": "HNoLQ38W8ImT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-threshold-5-llmhead-fp16 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "EDqX4vVjij1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wandb logs\n",
        "\n",
        "!wandb login\n",
        "\n",
        "# !export WANDB_PROJECT=\"SmolQuant\"\n",
        "# !export WANDB_NAME=\"opt-350m-eval\"\n",
        "\n"
      ],
      "metadata": {
        "id": "MA9CSo0Ycow1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evals on the zero-shot NLP tasks following OPT Paper"
      ],
      "metadata": {
        "id": "nC_ezXTZVQ_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=facebook/opt-350m \\\n",
        "    --tasks hellaswag,piqa,arc_easy,arc_challenge,openbookqa,winogrande,super-glue-lm-eval-v1 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto \\\n",
        "    --output_path ./results \\\n",
        "    --wandb_args project=\"SmolQuant\",entity=\"rentio\""
      ],
      "metadata": {
        "id": "oBNmBtEA8I3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now 8bit\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-bnb \\\n",
        "    --tasks hellaswag,piqa,arc_easy,arc_challenge,openbookqa,winogrande,super-glue-lm-eval-v1 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto \\\n",
        "    --output_path ./results \\\n",
        "    --wandb_args project=\"SmolQuant\",entity=\"rentio\""
      ],
      "metadata": {
        "id": "hjD3ymIWxAVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmqLt3hyxJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !lm-eval --tasks list"
      ],
      "metadata": {
        "id": "MDHae2br8Jad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking weights distribution\n"
      ],
      "metadata": {
        "id": "-zjVsXtU2g2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this script as: analyze_weights.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"facebook/opt-350m\"\n",
        "DEVICE = \"cpu\"\n",
        "DEFAULT_THRESHOLD = 6.0\n",
        "\n",
        "def analyze_model_weights(model_id):\n",
        "    \"\"\"\n",
        "    Loads a model and generates a grid of histograms and heatmaps for the\n",
        "    weights of each linear layer.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Part 1: Weight Analysis for {model_id} ---\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    linear_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, torch.nn.Linear)]\n",
        "    num_layers = len(linear_layers)\n",
        "    print(f\"Found {num_layers} linear layers.\")\n",
        "\n",
        "    fig, axes = plt.subplots(num_layers, 2, figsize=(15, num_layers * 5))\n",
        "    fig.suptitle(f'Static Weight Analysis for {model_id}', fontsize=20, y=1.0)\n",
        "\n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        weights = layer.weight.data\n",
        "        weights = weights.cpu().numpy()\n",
        "\n",
        "        # Plot Histogram\n",
        "        ax_hist = axes[i, 0]\n",
        "        ax_hist.hist(weights.flatten(), bins=500, log=True, color='darkblue')\n",
        "        ax_hist.axvline(x=DEFAULT_THRESHOLD, color='r', linestyle='--', label=f'Threshold = {DEFAULT_THRESHOLD}')\n",
        "        ax_hist.set_title(f\"{name}\\nHistogram of Weights\")\n",
        "        ax_hist.set_ylabel(\"Frequency (Log)\")\n",
        "        ax_hist.legend()\n",
        "\n",
        "        # Plot Heatmap\n",
        "        ax_heatmap = axes[i, 1]\n",
        "        im = ax_heatmap.imshow(weights, aspect='auto', cmap='viridis')\n",
        "        ax_heatmap.set_title(f\"{name}\\nHeatmap of Weights\")\n",
        "        # break\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = f\"{model_id.replace('/', '_')}_WEIGHT_analysis.png\"\n",
        "    plt.savefig(output_filename, dpi=150)\n",
        "    print(f\"\\nWeight analysis complete. Grid saved to: {output_filename}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    analyze_model_weights(MODEL_ID)"
      ],
      "metadata": {
        "id": "6dljfavlcCDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking activations distribution\n"
      ],
      "metadata": {
        "id": "LNu7lVdS2ldB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"facebook/opt-350m\"\n",
        "DATASET_ID = \"wikitext\"\n",
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
        "NUM_SAMPLES = 50  # Number of data samples to run through the model\n",
        "MAX_LENGTH = 512  # Sequence length\n",
        "DEVICE = \"cpu\"\n",
        "DEFAULT_THRESHOLD = 6.0\n",
        "\n",
        "# This dictionary will store the captured activations\n",
        "activation_storage = {}\n",
        "\n",
        "def get_hook(name):\n",
        "    \"\"\"\n",
        "    Creates a forward hook function to capture the input of a module.\n",
        "    The input to a linear layer is the activation from the previous layer.\n",
        "    \"\"\"\n",
        "    def hook(model, input, output):\n",
        "        # We store the first element of the input tuple, which is the hidden state tensor\n",
        "        activation_storage[name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "def analyze_model_activations(model_id):\n",
        "    \"\"\"\n",
        "    Loads a model and sample data, then uses forward hooks to capture and\n",
        "    visualize the input activations for each linear layer.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Part 2: Activation Analysis for {model_id} ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "    # --- 1. Register hooks on all linear layers ---\n",
        "    linear_layers = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            linear_layers.append((name, module))\n",
        "            module.register_forward_hook(get_hook(name))\n",
        "    num_layers = len(linear_layers)\n",
        "    print(f\"Registered forward hooks on {num_layers} linear layers.\")\n",
        "\n",
        "    # --- 2. Load data and run a forward pass to trigger hooks ---\n",
        "    dataset = load_dataset(DATASET_ID, DATASET_CONFIG, split=\"test\")\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(NUM_SAMPLES), desc=\"Running forward passes\"):\n",
        "            text = dataset[i]['text']\n",
        "            if not text: continue\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True).to(DEVICE)\n",
        "            model(**inputs) # This forward pass triggers the hooks\n",
        "    print(\"Forward passes complete. Activations have been captured.\")\n",
        "\n",
        "    # --- 3. Create the plot grid ---\n",
        "    fig, axes = plt.subplots(num_layers, 2, figsize=(15, num_layers * 5))\n",
        "    fig.suptitle(f'Dynamic Activation Analysis for {model_id}', fontsize=20, y=1.0)\n",
        "\n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        if name not in activation_storage:\n",
        "            print(f\"Warning: No activation captured for layer {name}\")\n",
        "            continue\n",
        "\n",
        "        activations = activation_storage[name]\n",
        "        abs_activations = activations.view(-1).cpu().numpy() # Flatten all activations over all samples\n",
        "\n",
        "        # Plot Histogram\n",
        "        ax_hist = axes[i, 0]\n",
        "        ax_hist.hist(abs_activations, bins=500, log=True, color='green')\n",
        "        ax_hist.axvline(x=DEFAULT_THRESHOLD, color='r', linestyle='--', label=f'Threshold = {DEFAULT_THRESHOLD}')\n",
        "        ax_hist.set_title(f\"{name}\\nHistogram of Activations\")\n",
        "        ax_hist.set_ylabel(\"Frequency (Log)\")\n",
        "        ax_hist.legend()\n",
        "\n",
        "        # Plot Heatmap\n",
        "        # We visualize the activations from the last sample for the heatmap\n",
        "        last_sample_activations = torch.abs(activations).cpu().numpy()\n",
        "        # Activations are 3D (batch, seq_len, features), we reshape to 2D\n",
        "        last_sample_activations_2d = last_sample_activations.reshape(-1, last_sample_activations.shape[-1])\n",
        "        ax_heatmap = axes[i, 1]\n",
        "        im = ax_heatmap.imshow(last_sample_activations_2d, aspect='auto', cmap='plasma')\n",
        "        ax_heatmap.set_title(f\"{name}\\nHeatmap of Activations (Last Sample)\")\n",
        "        ax_heatmap.set_xlabel(\"Feature Dimension\")\n",
        "        ax_heatmap.set_ylabel(\"Token Position (Flattened)\")\n",
        "\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = f\"{model_id.replace('/', '_')}_ACTIVATION_analysis.png\"\n",
        "    plt.savefig(output_filename, dpi=150)\n",
        "    print(f\"\\nActivation analysis complete. Grid saved to: {output_filename}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    analyze_model_activations(MODEL_ID)"
      ],
      "metadata": {
        "id": "OjqFuXdsedh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###now it takes all those NUM_SAMPLES and combined showsthe output\n"
      ],
      "metadata": {
        "id": "G1Hx5d8FWsi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"facebook/opt-350m\"\n",
        "DATASET_ID = \"wikitext\"\n",
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
        "NUM_SAMPLES = 50\n",
        "MAX_LENGTH = 512\n",
        "DEVICE = \"cpu\"\n",
        "DEFAULT_THRESHOLD = 6.0\n",
        "\n",
        "# MODIFICATION 1: Change storage to a dictionary of lists to collect all activations\n",
        "activation_storage = defaultdict(list)\n",
        "\n",
        "def get_hook(name):\n",
        "    \"\"\"Creates a hook function to capture and store inputs.\"\"\"\n",
        "    def hook(model, input, output):\n",
        "        # MODIFICATION 2: Append the activation to the list for this layer.\n",
        "        # Move to CPU immediately to save VRAM.\n",
        "        activation_storage[name].append(input[0].detach().cpu())\n",
        "    return hook\n",
        "\n",
        "def analyze_model_activations_aggregated(model_id):\n",
        "    print(f\"--- Starting Aggregated Activation Analysis for {model_id} ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "    linear_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, torch.nn.Linear)]\n",
        "    for name, module in linear_layers:\n",
        "        module.register_forward_hook(get_hook(name))\n",
        "    num_layers = len(linear_layers)\n",
        "    print(f\"Registered forward hooks on {num_layers} linear layers.\")\n",
        "\n",
        "    dataset = load_dataset(DATASET_ID, DATASET_CONFIG, split=\"test\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(NUM_SAMPLES), desc=\"Running forward passes\"):\n",
        "            text = dataset[i]['text']\n",
        "            if not text: continue\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True, padding=\"max_length\").to(DEVICE)\n",
        "            model(**inputs)\n",
        "    print(\"Forward passes complete. All activations have been captured.\")\n",
        "\n",
        "    fig, axes = plt.subplots(num_layers, 2, figsize=(15, num_layers * 5))\n",
        "    fig.suptitle(f'Aggregated Dynamic Activation Analysis ({NUM_SAMPLES} Samples) for {model_id}', fontsize=20, y=1.0)\n",
        "\n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        if name not in activation_storage:\n",
        "            continue\n",
        "\n",
        "        # MODIFICATION 3: Process the collected list of activation tensors\n",
        "        # Concatenate all tensors from the 50 samples into one giant tensor\n",
        "        all_activations_for_layer = torch.cat(activation_storage[name], dim=0)\n",
        "        abs_activations = torch.abs(all_activations_for_layer).view(-1).numpy() # Flatten for histogram\n",
        "\n",
        "        # --- Plot Histogram (Now represents ALL 50 samples) ---\n",
        "        ax_hist = axes[i, 0]\n",
        "        ax_hist.hist(abs_activations, bins=500, log=True, color='green')\n",
        "        ax_hist.axvline(x=DEFAULT_THRESHOLD, color='r', linestyle='--', label=f'Threshold = {DEFAULT_THRESHOLD}')\n",
        "        ax_hist.set_title(f\"{name}\\nHistogram of All Activations\")\n",
        "        ax_hist.set_ylabel(\"Frequency (Log)\")\n",
        "        ax_hist.legend()\n",
        "\n",
        "        # --- Plot Heatmap (Still shows the last sample for a clean visual) ---\n",
        "        last_sample_activations = torch.abs(activation_storage[name][-1]).numpy()\n",
        "        last_sample_activations_2d = last_sample_activations.reshape(-1, last_sample_activations.shape[-1])\n",
        "        ax_heatmap = axes[i, 1]\n",
        "        im = ax_heatmap.imshow(last_sample_activations_2d, aspect='auto', cmap='plasma')\n",
        "        ax_heatmap.set_title(f\"{name}\\nHeatmap of Activations (Last Sample)\")\n",
        "        ax_heatmap.set_xlabel(\"Feature Dimension\")\n",
        "        ax_heatmap.set_ylabel(\"Token Position (Flattened)\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = f\"{model_id.replace('/', '_')}_ACTIVATION_aggregated_analysis.png\"\n",
        "    plt.savefig(output_filename, dpi=150)\n",
        "    print(f\"\\nAggregated activation analysis complete. Grid saved to: {output_filename}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    analyze_model_activations_aggregated(MODEL_ID)"
      ],
      "metadata": {
        "id": "lv9Qt6DsWxZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xt4uxwBdXPAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}