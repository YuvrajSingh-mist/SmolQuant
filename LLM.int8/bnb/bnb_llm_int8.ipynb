{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "gw8jsQBPMagl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/lm-evaluation-harness\n",
        "!git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd /content/lm-evaluation-harness && pip install -e ."
      ],
      "metadata": {
        "id": "trMCIk6vM42X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdOn-wqqMV_7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_16bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    device_map=\"auto\",\n",
        "    # quantization_config=quantization_config,\n",
        "    dtype=\"auto\"\n",
        ")\n",
        "model_16bit.model.decoder.layers[-1].final_layer_norm.weight.dtype"
      ],
      "metadata": {
        "id": "kBOL_B6MUS6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_parameters(model):\n",
        "\n",
        "  for params in model.named_parameters():\n",
        "    print(\"Paramters name:\", params[0], \" | \", \"dtype: \", params[1].dtype)"
      ],
      "metadata": {
        "id": "-wm7le0kMpYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversion to 8bit using LLM.int8() method"
      ],
      "metadata": {
        "id": "HLZF6TLbOBE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        "    dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "KdtGQaj0OpR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eval fp16 model"
      ],
      "metadata": {
        "id": "qsXQuUkW07tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=facebook/opt-350m \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVlu7NgnSs1y",
        "outputId": "5c9604d1-17ea-4125-90c3-6021c302d8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-12 09:52:03.539895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760262723.597926    1290 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760262723.607841    1290 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760262723.647697    1290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760262723.647730    1290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760262723.647735    1290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760262723.647738    1290 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['hellaswag']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'facebook/opt-350m'}\n",
            "INFO:lm_eval.models.huggingface:Using device 'cuda:0'\n",
            "tokenizer_config.json: 100% 685/685 [00:00<00:00, 4.52MB/s]\n",
            "vocab.json: 899kB [00:00, 30.2MB/s]\n",
            "merges.txt: 456kB [00:00, 39.8MB/s]\n",
            "special_tokens_map.json: 100% 441/441 [00:00<00:00, 3.29MB/s]\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "README.md: 7.02kB [00:00, 26.3MB/s]\n",
            "data/train-00000-of-00001.parquet: 100% 24.4M/24.4M [00:00<00:00, 26.1MB/s]\n",
            "data/test-00000-of-00001.parquet: 100% 6.11M/6.11M [00:00<00:00, 18.7MB/s]\n",
            "data/validation-00000-of-00001.parquet: 100% 6.32M/6.32M [00:00<00:00, 20.8MB/s]\n",
            "Generating train split: 100% 39905/39905 [00:00<00:00, 106932.95 examples/s]\n",
            "Generating test split: 100% 10003/10003 [00:00<00:00, 128809.65 examples/s]\n",
            "Generating validation split: 100% 10042/10042 [00:00<00:00, 96377.76 examples/s]\n",
            "Map: 100% 39905/39905 [00:13<00:00, 2946.24 examples/s]\n",
            "Map: 100% 10042/10042 [00:02<00:00, 4549.65 examples/s]\n",
            "INFO:lm_eval.api.task:Building contexts for hellaswag on rank 0...\n",
            "100% 10042/10042 [00:05<00:00, 1826.39it/s]\n",
            "INFO:lm_eval.evaluator:Running loglikelihood requests\n",
            "Running loglikelihood requests:   0% 0/40168 [00:00<?, ?it/s]Passed argument batch_size = auto:1. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "Running loglikelihood requests:  43% 17301/40168 [01:13<01:17, 296.11it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a2a18f1"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer.push_to_hub(\"YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-llmhead-fp162\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval 8bit model"
      ],
      "metadata": {
        "id": "muHaV0N709uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "nQcyiTO41NFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Skipping lm head for int8"
      ],
      "metadata": {
        "id": "I8EL7mdKkX7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Extract parameters\n",
        "old_layer = model_8bit.model.decoder.project_out\n",
        "\n",
        "# Create a new fp16 Linear layer with same shape\n",
        "new_layer = torch.nn.Linear(old_layer.in_features, old_layer.out_features, bias=old_layer.bias is not None)\n",
        "new_layer = new_layer.to(torch.float16)\n",
        "\n",
        "# Copy weights (convert to fp16)\n",
        "new_layer.weight.data = old_layer.weight.data.clone().to(torch.float16)\n",
        "print(new_layer.weight.data.dtype)\n",
        "if old_layer.bias is not None:\n",
        "    new_layer.bias.data = old_layer.bias.data.clone().to(torch.float16)\n",
        "\n",
        "# Replace in model\n",
        "model_8bit.model.decoder.project_out = new_layer\n",
        "\n",
        "# model_8bit.model.decoder.project_out.weight.data = model_8bit.model.decoder.project_out.weight.data.to(torch.float16).clone()\n"
      ],
      "metadata": {
        "id": "rAqjG9U_nhqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "# model_8bit.model.decoder.project_out = model_8bit.model.decoder.project_out.clone().to(torch.float16).  # won't work cus the layer Linear8bitLt does not allow the dtype conversion to be successful"
      ],
      "metadata": {
        "id": "y8kzfHLUka8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_parameters(model_8bit)"
      ],
      "metadata": {
        "id": "LkUGBMuAO4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "168c2329"
      },
      "source": [
        "!pip install huggingface_hub -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking the memory before and after quantization\n"
      ],
      "metadata": {
        "id": "a4xeym-51pvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mem_16bit = model_16bit.get_memory_footprint() / 1e9"
      ],
      "metadata": {
        "id": "R737jOXsaS1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mem_8bit = model_8bit.get_memory_footprint() / 1e9"
      ],
      "metadata": {
        "id": "kRkKz7ZVaVPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Memory saved ratio"
      ],
      "metadata": {
        "id": "ZEe0JBexat6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(mem_8bit / mem_16bit)*100"
      ],
      "metadata": {
        "id": "843jvYMkavg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Uploading to hf\n"
      ],
      "metadata": {
        "id": "--OWSB5h1w58"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75c42576"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1c093c9"
      },
      "source": [
        "model_8bit.push_to_hub(\"facebook-opt-350m-8bit-llm.int8-llmhead-fp162\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evals (contd)"
      ],
      "metadata": {
        "id": "_MoYV-Us100y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model! (8bit without lm_head being in fp16)"
      ],
      "metadata": {
        "id": "CXH4Uqj8PKd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=YuvrajSingh9886/facebook-opt-350m-8bit-llm.int8-llmhead-fp162 \\\n",
        "    --tasks hellaswag \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto"
      ],
      "metadata": {
        "id": "XkFvzpG8PO2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this script as: analyze_weights.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"facebook/opt-350m\"\n",
        "DEVICE = \"cpu\"\n",
        "DEFAULT_THRESHOLD = 6.0\n",
        "\n",
        "def analyze_model_weights(model_id):\n",
        "    \"\"\"\n",
        "    Loads a model and generates a grid of histograms and heatmaps for the\n",
        "    weights of each linear layer.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Part 1: Weight Analysis for {model_id} ---\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
        "    print(\"Model loaded.\")\n",
        "\n",
        "    linear_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, torch.nn.Linear)]\n",
        "    num_layers = len(linear_layers)\n",
        "    print(f\"Found {num_layers} linear layers.\")\n",
        "\n",
        "    fig, axes = plt.subplots(num_layers, 2, figsize=(15, num_layers * 5))\n",
        "    fig.suptitle(f'Static Weight Analysis for {model_id}', fontsize=20, y=1.0)\n",
        "\n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        weights = layer.weight.data\n",
        "        weights = weights.cpu().numpy()\n",
        "\n",
        "        # Plot Histogram\n",
        "        ax_hist = axes[i, 0]\n",
        "        ax_hist.hist(weights.flatten(), bins=500, log=True, color='darkblue')\n",
        "        ax_hist.axvline(x=DEFAULT_THRESHOLD, color='r', linestyle='--', label=f'Threshold = {DEFAULT_THRESHOLD}')\n",
        "        ax_hist.set_title(f\"{name}\\nHistogram of Weights\")\n",
        "        ax_hist.set_ylabel(\"Frequency (Log)\")\n",
        "        ax_hist.legend()\n",
        "\n",
        "        # Plot Heatmap\n",
        "        ax_heatmap = axes[i, 1]\n",
        "        im = ax_heatmap.imshow(weights, aspect='auto', cmap='viridis')\n",
        "        ax_heatmap.set_title(f\"{name}\\nHeatmap of Weights\")\n",
        "        # break\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = f\"{model_id.replace('/', '_')}_WEIGHT_analysis.png\"\n",
        "    plt.savefig(output_filename, dpi=150)\n",
        "    print(f\"\\nWeight analysis complete. Grid saved to: {output_filename}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    analyze_model_weights(MODEL_ID)"
      ],
      "metadata": {
        "id": "6dljfavlcCDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this script as: analyze_activations.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"facebook/opt-350m\"\n",
        "DATASET_ID = \"wikitext\"\n",
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
        "NUM_SAMPLES = 50  # Number of data samples to run through the model\n",
        "MAX_LENGTH = 512  # Sequence length\n",
        "DEVICE = \"cpu\"\n",
        "DEFAULT_THRESHOLD = 6.0\n",
        "\n",
        "# This dictionary will store the captured activations\n",
        "activation_storage = {}\n",
        "\n",
        "def get_hook(name):\n",
        "    \"\"\"\n",
        "    Creates a forward hook function to capture the input of a module.\n",
        "    The input to a linear layer is the activation from the previous layer.\n",
        "    \"\"\"\n",
        "    def hook(model, input, output):\n",
        "        # We store the first element of the input tuple, which is the hidden state tensor\n",
        "        activation_storage[name] = input[0].detach()\n",
        "    return hook\n",
        "\n",
        "def analyze_model_activations(model_id):\n",
        "    \"\"\"\n",
        "    Loads a model and sample data, then uses forward hooks to capture and\n",
        "    visualize the input activations for each linear layer.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Part 2: Activation Analysis for {model_id} ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(DEVICE)\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "    # --- 1. Register hooks on all linear layers ---\n",
        "    linear_layers = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            linear_layers.append((name, module))\n",
        "            module.register_forward_hook(get_hook(name))\n",
        "    num_layers = len(linear_layers)\n",
        "    print(f\"Registered forward hooks on {num_layers} linear layers.\")\n",
        "\n",
        "    # --- 2. Load data and run a forward pass to trigger hooks ---\n",
        "    dataset = load_dataset(DATASET_ID, DATASET_CONFIG, split=\"test\")\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(NUM_SAMPLES), desc=\"Running forward passes\"):\n",
        "            text = dataset[i]['text']\n",
        "            if not text: continue\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True).to(DEVICE)\n",
        "            model(**inputs) # This forward pass triggers the hooks\n",
        "    print(\"Forward passes complete. Activations have been captured.\")\n",
        "\n",
        "    # --- 3. Create the plot grid ---\n",
        "    fig, axes = plt.subplots(num_layers, 2, figsize=(15, num_layers * 5))\n",
        "    fig.suptitle(f'Dynamic Activation Analysis for {model_id}', fontsize=20, y=1.0)\n",
        "\n",
        "    for i, (name, layer) in enumerate(linear_layers):\n",
        "        if name not in activation_storage:\n",
        "            print(f\"Warning: No activation captured for layer {name}\")\n",
        "            continue\n",
        "\n",
        "        activations = activation_storage[name]\n",
        "        abs_activations = activations.view(-1).cpu().numpy() # Flatten all activations over all samples\n",
        "\n",
        "        # Plot Histogram\n",
        "        ax_hist = axes[i, 0]\n",
        "        ax_hist.hist(abs_activations, bins=500, log=True, color='green')\n",
        "        ax_hist.axvline(x=DEFAULT_THRESHOLD, color='r', linestyle='--', label=f'Threshold = {DEFAULT_THRESHOLD}')\n",
        "        ax_hist.set_title(f\"{name}\\nHistogram of Activations\")\n",
        "        ax_hist.set_ylabel(\"Frequency (Log)\")\n",
        "        ax_hist.legend()\n",
        "\n",
        "        # Plot Heatmap\n",
        "        # We visualize the activations from the last sample for the heatmap\n",
        "        last_sample_activations = torch.abs(activations).cpu().numpy()\n",
        "        # Activations are 3D (batch, seq_len, features), we reshape to 2D\n",
        "        last_sample_activations_2d = last_sample_activations.reshape(-1, last_sample_activations.shape[-1])\n",
        "        ax_heatmap = axes[i, 1]\n",
        "        im = ax_heatmap.imshow(last_sample_activations_2d, aspect='auto', cmap='plasma')\n",
        "        ax_heatmap.set_title(f\"{name}\\nHeatmap of Activations (Last Sample)\")\n",
        "        ax_heatmap.set_xlabel(\"Feature Dimension\")\n",
        "        ax_heatmap.set_ylabel(\"Token Position (Flattened)\")\n",
        "\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
        "    output_filename = f\"{model_id.replace('/', '_')}_ACTIVATION_analysis.png\"\n",
        "    plt.savefig(output_filename, dpi=150)\n",
        "    print(f\"\\nActivation analysis complete. Grid saved to: {output_filename}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    analyze_model_activations(MODEL_ID)"
      ],
      "metadata": {
        "id": "OjqFuXdsedh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"bigscience/bloom-1b7\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    llm_int8_threshold=0.0,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ],
      "metadata": {
        "id": "rx5l1lodWG7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}